{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Data Analysis III\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Agenda:**\n",
    "\n",
    "    * CProfile \n",
    "    * Cython\n",
    "    * sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing programs is fun, but making them fast can be a pain. Python programs are no exception to that, but the basic profiling toolchain is actually not that complicated to use. Here, I would like to show you how you can quickly profile and analyze your Python code to find what part of the code you should optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do profiling manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting line_profiler\n",
      "  Downloading https://files.pythonhosted.org/packages/14/fc/ecf4e238bb601ff829068e5a72cd1bd67b0ee0ae379db172eb6a0779c6b6/line_profiler-2.1.2.tar.gz (83kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 1.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting IPython>=0.13 (from line_profiler)\n",
      "  Downloading https://files.pythonhosted.org/packages/b1/7f/91d50f28af3e3a24342561983a7857e399ce24093876e6970b986a0b6677/ipython-6.4.0-py3-none-any.whl (750kB)\n",
      "\u001b[K    100% |████████████████████████████████| 757kB 866kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting pickleshare (from IPython>=0.13->line_profiler)\n",
      "  Downloading https://files.pythonhosted.org/packages/9f/17/daa142fc9be6b76f26f24eeeb9a138940671490b91cb5587393f297c8317/pickleshare-0.7.4-py2.py3-none-any.whl\n",
      "Collecting pexpect; sys_platform != \"win32\" (from IPython>=0.13->line_profiler)\n",
      "  Downloading https://files.pythonhosted.org/packages/89/e6/b5a1de8b0cc4e07ca1b305a4fcc3f9806025c1b651ea302646341222f88b/pexpect-4.6.0-py2.py3-none-any.whl (57kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 1.7MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting backcall (from IPython>=0.13->line_profiler)\n",
      "  Downloading https://files.pythonhosted.org/packages/84/71/c8ca4f5bb1e08401b916c68003acf0a0655df935d74d93bf3f3364b310e0/backcall-0.1.0.tar.gz\n",
      "Collecting decorator (from IPython>=0.13->line_profiler)\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/bb/a24838832ba35baf52f32ab1a49b906b5f82fb7c76b2f6a7e35e140bac30/decorator-4.3.0-py2.py3-none-any.whl\n",
      "Collecting pygments (from IPython>=0.13->line_profiler)\n",
      "  Downloading https://files.pythonhosted.org/packages/02/ee/b6e02dc6529e82b75bb06823ff7d005b141037cb1416b10c6f00fc419dca/Pygments-2.2.0-py2.py3-none-any.whl (841kB)\n",
      "\u001b[K    94% |██████████████████████████████▍ | 798kB 1.3MB/s eta 0:00:01"
     ]
    }
   ],
   "source": [
    "!pip3 install line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile speedup.py\n",
    "\n",
    "import random\n",
    "\n",
    "class Matrix(list):\n",
    "    @classmethod\n",
    "    def zeros(cls, shape):\n",
    "        n_rows, n_cols = shape\n",
    "        return cls([[0] * n_cols for i in range(n_rows)])\n",
    "\n",
    "    @classmethod\n",
    "    def random(cls, shape):\n",
    "        M, (n_rows, n_cols) = cls(), shape \n",
    "        for i in range (n_rows):\n",
    "            M.append([random.randint(-255, 255) for j in range (n_cols)])\n",
    "        return M\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return ((0, 0) if not self else (len(self), len(self[0])))\n",
    "    \n",
    "    \n",
    "def dot_product(X, Y):\n",
    "    n_xrows, n_xcols = X.shape\n",
    "    n_yrows, n_ycols = Y.shape\n",
    "    Z = Matrix.zeros((n_xrows, n_ycols))\n",
    "    for i in range(n_xrows):\n",
    "        for j in range(n_xcols):\n",
    "            for k in range(n_ycols):\n",
    "                Z[i][k] += X[i][j] * Y[j][k]\n",
    "    return Z\n",
    "\n",
    "def bench(shape=(64, 64), n_iter=16):\n",
    "    X = Matrix.random(shape)\n",
    "    Y = Matrix.random(shape)\n",
    "    for iter in range(n_iter):\n",
    "        dot_product(X, Y)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bench()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "a1 = np.random.rand(3,2)\n",
    "a2 = np.random.rand(2,3)\n",
    "a1.dot(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cProfile module allows you to profile Python code up to a function or method call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "\n",
    "source = open(\"speedup.py\").read()\n",
    "cProfile.run(source, sort=\"tottime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speedup import dot_product, bench\n",
    "%lprun -f dot_product bench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Scikit-Learn` is a library, in which implemented a large number of machine learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can separate learning problems in a few large categories:\n",
    "\n",
    "1. supervised learning, in which the data comes with additional attributes that we want to predict.This problem can be either:\n",
    "\n",
    "    - classification: samples belong to two or more classes and we want to learn from already labeled data how to predict the class of unlabeled data.\n",
    "    - regression: if the desired output consists of one or more continuous variables, then the task is called regression.\n",
    "\n",
    "2. unsupervised learning, in which the training data consists of a set of input vectors x without any corresponding target values. The goal in such problems may be to discover groups of similar examples within the data, where it is called clustering, or to determine the distribution of data within the input space, known as density estimation, or to project the data from a high-dimensional space down to two or three dimensions for the purpose of visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, a learning problem considers a set of `n` samples of data and then tries to predict properties of unknown data. If each sample is more than a single number and, for instance, a multi-dimensional entry (aka multivariate data), it is said to have several attributes or features.\n",
    "\n",
    "This idea of first learn known samples and then predict new samples is implemented in scikit-learn with two basic functions: `fit` and `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Churn-Modelling.csv')\n",
    "df.dropna(inplace=True)\n",
    "df = df[['CreditScore', 'Age', 'Balance', 'EstimatedSalary', 'Exited']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everyone is familiar with the fact that most gradient methods (strongly or strangely to scale data). Therefore, before running algorithms, either normalization or so-called standardization is usually done. Normalization involves replacing the nominal characteristics so that each of them lies in the range from 0 to 1. Standardization implies the same preprocessing of data, after which each attribute has an average of 0 and a variance of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "# normalize the data attributes\n",
    "normalized_df = preprocessing.normalize(df)\n",
    "# standardize the data attributes\n",
    "standardized_df = preprocessing.scale(df) # Standardization isn't required for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training set and testing set\n",
    "\n",
    "Machine learning is about learning some properties of a data set and applying them to new data. This is why a common practice in machine learning to evaluate an algorithm is to split the data at hand into two sets, one that we call the training set on which we learn data properties and one that we call the testing set on which we test these properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train,test = train_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(train[['CreditScore', 'Age', 'Balance', 'EstimatedSalary']], train['Exited'])\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = test['Exited']\n",
    "predicted = model.predict(test[['CreditScore', 'Age', 'Balance', 'EstimatedSalary']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `confusion_matrix()` function will calculate a confusion matrix and return the result as an array.\n",
    "The result is telling us that we have 1927+29 correct predictions and 492+47 incorrect predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/documentation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
